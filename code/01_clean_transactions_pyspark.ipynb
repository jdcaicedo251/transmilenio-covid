{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Transactions \n",
    "\n",
    "This notebook details the process to clean all the transactions data. There are two main objectives. \n",
    "- 1. Eliminate certain transactions(E.g., Transfers, double validations)\n",
    "- 2. Standarize stations. \n",
    "\n",
    "All changes are supported by a preliminary analysis of the data contain in the notebook: \"estaciones_preguntas\"\n",
    "\n",
    "The final product will be a function to clean transactions that can be use across all other analysis notebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext(appName=\"transactions_cleaning\")\n",
    "\n",
    "from pyspark.sql import * #This enables the SparkSession object\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"transactions_cleaning\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import time \n",
    "# import random \n",
    "# import pandas as pd \n",
    "# import numpy as np \n",
    "\n",
    "# import warnings\n",
    "# import glob\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import pow, col, sqrt\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping transfers \n",
    "There are 3 types of transfers that will be dropped: \n",
    "\n",
    "- Transactions that are transfers (Cost of transactions is zero)\n",
    "- Transactions that by the same card and the same station within 30 mins \n",
    "- Transactions at stations and access points that seems to be transfers as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datetime_col(df):\n",
    "    df = df.withColumn(\"datetime_str\", concat(col('fechatransaccion'),lit(' '), col('horatransaccion')))\n",
    "    return df.withColumn(\"dt\", to_timestamp(df.datetime_str, 'yyyyMMdd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_transactions(df, station, access_point = None):\n",
    "    \"\"\" Drops the transactions done in an given station and access point(optional)\n",
    "    Input: \n",
    "    - df: PySpark DataFrame. Raw transactions\n",
    "    - stations: 'str'. A string that helps identify the station. \n",
    "    - access_point: 'str'. A string that helps identify the access point. \n",
    "    \"\"\"\n",
    "    \n",
    "    if access_point is not None:\n",
    "        station_name = df.nombreestacion.contains(station)\n",
    "        entrance = df.nombreaccesoestacion.contains(access_point)\n",
    "        filters = station_name & entrance\n",
    "    else: \n",
    "        filters = df.nombreestacion.contains(station)\n",
    "    return df.filter(~filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_transafers(df):\n",
    "    \"\"\" Drops all transfer transaction within the system. A tranfer transaction has valor = 0\"\"\"\n",
    "    return df.filter(~(df.valor == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(df):\n",
    "    \"\"\" Drops transactions made by the same id in the same station within 30mins\"\"\"\n",
    "    w = Window().partitionBy(col(\"card_id\")).orderBy(col(\"dt\") ) \n",
    "    df2 = df.select(\"*\", lag(df.nombreestacion).over(w).alias(\"lag_station\"))\n",
    "    df2 = df2.select(\"*\", lag(df.dt).over(w).alias(\"lag_dt\"))\n",
    "    df2 = df2.withColumn(\"diff_seconds\", col(\"dt\").cast(\"long\")- col(\"lag_dt\").cast(\"long\"))\n",
    "    df2 = df2.withColumn(\"same_station\", col('nombreestacion') == col('lag_station'))\n",
    "    \n",
    "    mask = (df2.same_station) & (df2.diff_seconds < 1800)\n",
    "    df2 = df2.withColumn(\"drop_value\", mask).na.fill(False)\n",
    "    return df2.filter(~df2.drop_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transactions(transactions):\n",
    "    \"\"\" Clean raw transactions data by eliminating transactions transfers and duplicates\"\"\"\n",
    "    \n",
    "    #Preprocessing\n",
    "    df = create_datetime_col(transactions)\n",
    "    \n",
    "    #Dropping transactions at stations and entry point \n",
    "    to_drop_transactions = {\"02000\" : \"(09) PLATAFORMA 2 DESALIMENTACI\", \"10000\" : \"(02) DESALIME\",\n",
    "                        \"09000\" : \"DESALIME\", \"05100\" : \"(MA) DESALIMENTACION CASTIL\", \n",
    "                        \"10005\" : \"(04) PISO 2 DESALIME\", \"08000\" : \"(05) DESALIME\", \n",
    "                        \"08100\" : None, \"40000\" : None, \"22000\": None} #22000 is \"Estacion virtual\"\n",
    "    \n",
    "#     cols = ['fechaclearing','fechatransaccion','horatransaccion','nombrefase', 'nombreemisor','nombrelinea',\n",
    "#             'nombreestacion','nombreaccesoestacion', 'nombredispositivo','nombreperfil','numerotarjeta',\n",
    "#             'valor','nombretipotarjeta', 'dt', 'diff_seconds']\n",
    "#     cols = ['dt', 'nombreestacion','nombreaccesoestacion','numerotarjeta',\n",
    "#             'valor' , 'diff_seconds']\n",
    "\n",
    "    for key, value in to_drop_transactions.items():\n",
    "        df = drop_transactions(df, key, value)\n",
    "        \n",
    "    df = drop_transafers(df)\n",
    "    df = drop_duplicates(df)\n",
    "    return df#.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stations(transactions, stations):\n",
    "    \"\"\" Cleans station \"\"\"\n",
    "    df = transactions.join(stations, \n",
    "                        how = 'left', \n",
    "                        on = ['nombreestacion','nombreaccesoestacion'])\n",
    "    \n",
    "    cols = ['card_id','dt','fechatransaccion','horatransaccion',\n",
    "            'station_name','valor','diff_seconds']\n",
    "    \n",
    "    return df.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_map(df, new_ids):\n",
    "    ''' Returns a dataframe with a new colum containing new_IDs\n",
    "    Input:\n",
    "    - dataFrame: PySpark dataframe with a column 'numerotarjeta' as ID\n",
    "    - new_ids: mapping of 'numerotarjetas' with a new (simple/shorter) id\n",
    "    '''\n",
    "    \n",
    "    cols = ['card_id', 'fechatransaccion','horatransaccion','nombreestacion','nombreaccesoestacion','valor']\n",
    "    \n",
    "    return df.join(new_ids, on = 'numerotarjeta', how = 'inner').select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_path = \"../../../../../../gs_validaciones_tm/validaciones_tmsa/validacionTroncal/validacionTroncal20200218.csv\"\n",
    "input_path = '../data/input/'\n",
    "output_path = '../data/output/tables/'\n",
    "\n",
    "transactions = spark.read.option(\"header\", \"true\").option(\"sep\", \";\").csv(transactions_path)\n",
    "id_mapping = spark.read.csv(output_path + 'id_mapping.csv', header =True, sep = ',')\n",
    "stations = spark.read.csv(input_path + 'stations.csv', header =True, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_transactions = transactions.transform(lambda df: id_map(df, id_mapping)) \\\n",
    "                                 .transform(lambda df: clean_transactions(df)) \\\n",
    "                                 .transform(lambda df: clean_stations(df, stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.9 ms, sys: 45.1 ms, total: 124 ms\n",
      "Wall time: 15min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_transactions.write.csv(output_path + 'transactions.csv', mode = 'overwrite', header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
