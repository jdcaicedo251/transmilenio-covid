{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home - work - other inference \n",
    "This notebook contains the detail information to infer home, work and others stations per user.\n",
    "\n",
    "This is a rule-based approach with the following rules: \n",
    "- Home: Most common first transaction of the day\n",
    "before noon.\n",
    "- Work: Most common transaction such that difference\n",
    "between consecutive transaction in the same day is\n",
    "greater than 6 h.\n",
    "- Other: Transactions in any other station that are not\n",
    "classified as home or work. \n",
    "\n",
    "We use row_number() algorith to rank 1,2,3,4 (no ties if two stations have the same number of transactions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql.functions import *\n",
    "sc = ps.SparkContext(appName=\"home_work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import time \n",
    "import random \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import * #This enables the SparkSession object\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import pow, col, sqrt\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"home_work\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process \n",
    "- map new ids \n",
    "- filter frequent users (I would need to create a frequent/non-frequen users file again with the new IDs)[before march 8]\n",
    "- filter out observation after march 8. We will work only with observations between Jan-01-2020, and March-8-2020. \n",
    "- Merge with clean stations as well. \n",
    "- Find ranks per ID \n",
    "- Create a file with the following columns: ID, Home_station, Work_station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_format(df, date_col = 'fechatransaccion', time_col = \"horatransaccion\"):\n",
    "    ''' Returns dataframe with date and time as a timespamt format\n",
    "    Input: \n",
    "     - df: PySpark dataframe: Raw transactions\n",
    "     - date_col: str. Column name for date \n",
    "     - time_col: str. Column name for time\n",
    "     \n",
    "     Return: \n",
    "     PySpark dataframe. \n",
    "     Time col:\n",
    "     - timestamp: time in 'to_timestamp' format '''\n",
    "    # add time\n",
    "    df = df.withColumn(\"timestamp\",\n",
    "        to_timestamp(col(time_col),\"HH:mm:ss\"))\\\n",
    "        .withColumn(\"hour\", hour(col(\"timestamp\")))\\\n",
    "        .withColumn(\"minute\", minute(col(\"timestamp\")))\\\n",
    "        .withColumn(\"second\", second(col(\"timestamp\")))\\\n",
    "        .withColumn('time', col('hour') + col('minute')/60 + col('second')/3600)\n",
    "    \n",
    "    # add date\n",
    "    df = df.withColumn('date', to_date(unix_timestamp(col(date_col), 'yyyyMMdd').cast(\"timestamp\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proccess(df):\n",
    "    ''' Preproccess transactions data '''\n",
    "    \n",
    "    df_ = df.transform(lambda df: time_format(df))\\\n",
    "                  .select(['card_id', 'date', 'time', 'station_name'])\\\n",
    "                  .orderBy(['card_id', 'date', 'time'])\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def home_location(df):\n",
    "    '''Estimates the home location of a card_id'''\n",
    "    \n",
    "    w = Window().partitionBy(col(\"card_id\")).orderBy(col(\"date\") ) \n",
    "    \n",
    "    df = df.select(\"*\", lag(df.date).over(w).alias(\"lag_date\"))\\\n",
    "            .withColumn('first_day', ~(col('date') == col('lag_date')))\\\n",
    "            .na.fill(value = True)\\\n",
    "            .withColumn('first_day_noon', (col('first_day')) & (col('time')<12))\\\n",
    "            .filter(col('first_day_noon'))\\\n",
    "            .groupBy(['card_id', 'station_name'])\\\n",
    "            .agg({'date':'count', 'time': 'mean'})\\\n",
    "            .withColumn('rank',row_number().over(Window.partitionBy(\"card_id\").orderBy(desc(\"count(date)\"))))\\\n",
    "            .filter(col('rank')==1)\n",
    "    \n",
    "    final_df = df.select(col('card_id'),\n",
    "                         col('station_name').alias('home_station'), \n",
    "                         col('count(date)').alias('transacciones_h'), \n",
    "                         col('avg(time)').alias('time_h'))\n",
    "    \n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_location(df):\n",
    "    w = Window().partitionBy(col(\"card_id\"), col('date')).orderBy(col(\"time\") ) \n",
    "    \n",
    "    df = df.select(\"*\", lag(df.time).over(w).alias(\"lag_time\")).dropna(subset = ('lag_time'))\\\n",
    "            .withColumn('time_difference', col('time') - col('lag_time'))\\\n",
    "            .filter(col('time_difference')>6)\\\n",
    "            .groupBy(['card_id', 'station_name'])\\\n",
    "            .agg({'date':'count', 'time': 'mean'})\\\n",
    "            .withColumn('rank',row_number().over(Window.partitionBy(\"card_id\").orderBy(desc(\"count(date)\"))))\\\n",
    "            .filter(col('rank')==1)\n",
    "    \n",
    "    final_df = df.select(col('card_id'),\n",
    "                        col('station_name').alias('work_station'), \n",
    "                        col('count(date)').alias('transacciones_w'), \n",
    "                        col('avg(time)').alias('time_w'))\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def home_work_inference(transactions):\n",
    "    \"\"\" Infer home and work stations for a given tranvel sequence\"\"\"\n",
    "    df_p = transactions\n",
    "    home = home_location(df_p)#.show()\n",
    "    work = work_location(df_p)\n",
    "    \n",
    "    return home.join(work, on = 'card_id', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../data/output/'\n",
    "input_path = '../data/input/'\n",
    "\n",
    "transactions = spark.read.csv(output_path + 'tables/transactions_frequent_users.csv',\n",
    "                              header =True, sep = ',')\n",
    "\n",
    "transactions = pre_proccess(transactions)\n",
    "hw_location = home_work_inference(transactions).cache()\n",
    "# hw_location.to_csv(output_path + 'tables/hw_#location_v2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------+------------------+--------------------+---------------+------------------+\n",
      "| card_id|        home_station|transacciones_h|            time_h|        work_station|transacciones_w|            time_w|\n",
      "+--------+--------------------+---------------+------------------+--------------------+---------------+------------------+\n",
      "|10000172|(04000) Cabecera ...|            135| 8.935946502057607|     (04108) El Polo|             38| 18.43856725146199|\n",
      "|10000454|    (07005) ALQUERIA|             87|  8.76148148148148|      (02302) Virrey|             62|17.322526881720428|\n",
      "|10000472|    (09002) Consuelo|            139| 8.235349720223823|  (04103) Las Ferias|             43|17.980458656330754|\n",
      "|10000591|    (02502) Terminal|             88| 8.217159090909092|      (02200) Alcalá|             61|17.789339708561016|\n",
      "|10000720|(50003) Corral Mo...|             34| 6.635457516339869|(02000) Cabecera ...|             16| 17.15161458333333|\n",
      "|10000723|   (03013) RIO NEGRO|             13| 7.547350427350427|(06111) Universid...|              1|19.448888888888888|\n",
      "|10000761|    (07504) TERREROS|            212| 7.202198637316562|(06102) Salitre E...|            133|18.191244778613203|\n",
      "|10001922|(05000) Portal Am...|             48| 6.476435185185182|(06106) Recinto F...|             14|16.000734126984128|\n",
      "|10001989|(10000) Portal 20...|             21| 8.385608465608465|    (09117) Calle 45|              1|18.278055555555554|\n",
      "|10002280|    (04001) Quirigua|             48| 9.558096064814814|    (09119) Calle 57|             38|16.117573099415203|\n",
      "|10002674|(12002) CDS - Car...|              4| 9.934444444444445|(07111) NQS - RIC...|              2|13.896666666666667|\n",
      "|10002811|(08000) Portal Tu...|             10| 7.746138888888888| (09004) Santa Lucía|              2| 17.26222222222222|\n",
      "|10003840|  (04106) Carrera 47|             29| 6.544932950191568|    (09117) Calle 45|              4|17.796319444444443|\n",
      "|10004662|(03000) Portal Su...|             37|6.1138738738738745|(07113) SANTA ISABEL|             21|16.045687830687832|\n",
      "|10004786|(07006) General S...|              9| 6.573055555555555|    (02303) Calle 85|              9|16.815462962962965|\n",
      "|10004906|(03000) Portal Su...|             24| 7.522708333333334|  (07110) PALOQUEMAO|             16| 17.81822916666667|\n",
      "|10005071|(05000) Portal Am...|             76|  6.90596125730994|(04000) Cabecera ...|             41| 18.33376693766937|\n",
      "|10005243|(03002) SUBA - TV...|             16| 5.953958333333333|  (09121) Las Flores|             52| 21.28001068376068|\n",
      "|10005612|     (02101) Toberín|             33|10.320067340067338|   (02300) Calle 100|              4|19.499305555555555|\n",
      "|10005659|        (07010) Bosa|              3| 8.578148148148149|                null|           null|              null|\n",
      "+--------+--------------------+---------------+------------------+--------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 57 ms, sys: 34.1 ms, total: 91.1 ms\n",
      "Wall time: 10min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hw_location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 753 ms, total: 18.9 s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hw_location_final = hw_location.toPandas()\n",
    "hw_location_final.to_csv(output_path + 'tables/hw_location.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
